jobs:
- job: run_profile_collection_testing

  pool:
    vmImage: 'ubuntu-latest'

  displayName: "profile_collection (${{ parameters.beamline_acronym }}):"

  variables:
    MPLBACKEND: Qt5Agg
    TEST_PROFILE: test
    CONDA_ENV_NAME: collection-2020-2.0rc8
    # DOI: https://doi.org/10.5281/zenodo.4057063
    # URL: https://zenodo.org/record/4057063/files/collection-2020-2.0rc8.tar.gz?download=1
    ZENODO_ID: 4057063
    MD5_CHECKSUM: db8fb77565afaa7a4c75ddcf4c5f839b
    # Ophyd/EPICS vars
    OPHYD_TIMEOUT: 60
    EPICS_CA_AUTO_ADDR_LIST: NO
    EPICS_CA_ADDR_LIST: 127.0.0.1
    USE_EPICS_IOC: 0

  strategy:
    matrix:
      py37_pyepics:
        OPHYD_CONTROL_LAYER: pyepics
        CONDA_CHANNEL_NAME: nsls2forge
        PYTHON_VERSION: 3.7
      py37_caproto:
        OPHYD_CONTROL_LAYER: caproto
        CONDA_CHANNEL_NAME: nsls2forge
        PYTHON_VERSION: 3.7

  steps:

  # Check the env
  - script: env | sort -u
    displayName: "* check the env"
    condition: succeeded()

  # TODO: uncomment the block below once all profile repos are
  # "profile_collection". Some of them are not compliant with the name and
  # single profile per repo approach. See the list of those below.
  # # Check the startup/ dir exists
  # - script: |
  #     if [ ! -d "startup/" ]; then
  #         echo "No startup/ dir found in $PWD"
  #         exit 9
  #     fi
  #   displayName: "* check the startup/ dir exists"
  #   condition: succeeded()

  # Get pyOlog and databroker configs
  - script: |
      set -e
      wget https://raw.githubusercontent.com/NSLS-II/profile-collection-ci/master/configs/pyOlog.conf -O ~/pyOlog.conf

      mkdir -v -p ~/.config/databroker/
      wget https://raw.githubusercontent.com/NSLS-II/profile-collection-ci/master/configs/databroker.yml -O ~/.config/databroker/_legacy_config.yml
      wget https://raw.githubusercontent.com/NSLS-II/profile-collection-ci/master/configs/databroker.yml -O ~/.config/databroker/$(echo ${{ parameters.beamline_acronym }} | awk '{print tolower($1)}').yml
    displayName: "* get pyOlog and databroker configs"
    condition: succeeded()

  # Prepare a test profile dir
  - script: |
      set -e

      mkdir -v -p ~/.ipython/profile_${TEST_PROFILE}

      # Copy the entire "startup" directory to the test profile.
      cp -rv startup ~/.ipython/profile_${TEST_PROFILE}/
      # Remove the top-level .py files as they will be loaded separately from
      # profile_collection/startup dir. That allows proper capturing of the
      # exit code in case IPython startup files have issues.
      rm -fv ~/.ipython/profile_${TEST_PROFILE}/startup/*.py
    displayName: "* prepare a test profile dir"
    condition: succeeded()

  # Check services and system packages
  - script: |
      sudo systemctl status
      apt list --installed
    displayName: "* check services and system packages"
    condition: succeeded()

  # In Ubuntu 18.04/bionic the mongodb-org and xvfb packages are already
  # preinstalled.
  # # Install required packages
  # - script: |
  #     sudo apt-get clean
  #     sudo apt-get update
  #     sudo apt-get install xvfb
  #   displayName: "* install required packages"
  #   condition: succeeded()

  # Start mongodb service
  - script: |
      sudo systemctl start mongod && sudo systemctl status mongod
    displayName: "* start mongodb service"
    condition: succeeded()

  # Start docker service
  - script: |
      sudo systemctl start docker && sudo systemctl status docker
    displayName: "* start docker service"
    condition: succeeded()

  # Download and install miniconda
  - script: |
      set -e
      wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
      bash ./miniconda.sh -b -p $HOME/miniconda
      source $HOME/miniconda/etc/profile.d/conda.sh  # this makes all conda vars available
      conda update conda --yes
    displayName: "* download and install miniconda"
    condition: succeeded()

  # Create conda env
  - script: |
      set -e
      source $HOME/miniconda/etc/profile.d/conda.sh  # this makes all conda vars available
      wget -q https://zenodo.org/record/${ZENODO_ID}/files/${CONDA_ENV_NAME}.tar.gz?download=1 -O ${CONDA_ENV_NAME}.tar.gz
      echo "${MD5_CHECKSUM}  ${CONDA_ENV_NAME}.tar.gz" > checksum.txt
      md5sum --check checksum.txt
      mkdir -p $HOME/miniconda/envs/${CONDA_ENV_NAME}
      tar -xf ${CONDA_ENV_NAME}.tar.gz -C $HOME/miniconda/envs/${CONDA_ENV_NAME}
      conda activate $HOME/miniconda/envs/${CONDA_ENV_NAME}
      conda unpack

      conda info -a
      conda env list
      conda list
      pip list
    displayName: "* create test conda env"
    condition: succeeded()

  # Perform beamline-specific actions
  - script: |
      set -e
      source $HOME/miniconda/etc/profile.d/conda.sh  # this makes all conda vars available
      conda activate ${CONDA_ENV_NAME}
      if [ -f .ci/bl-specific.sh ]; then
          cat .ci/bl-specific.sh
          . .ci/bl-specific.sh
      fi
      env | sort -u
    displayName: "* perform beamline-specific actions"
    condition: succeeded()

  # Optionally start caproto IOC and start IPython with startup files
  - script: |
      set -e
      # Start Xvfb
      # (from https://developercommunity.visualstudio.com/content/problem/336288/headless-testing-using-xvfb-on-hosted-ubuntu-1604.html)
      /sbin/start-stop-daemon --start --quiet --pidfile /tmp/custom_xvfb_99.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -ac -screen 0 1280x1024x16
      export DISPLAY=:99

      # Activate the test env and display some relevant info:
      source $HOME/miniconda/etc/profile.d/conda.sh  # this makes all conda vars available
      conda activate ${CONDA_ENV_NAME}
      conda env list
      conda info
      conda list

      env | sort -u

      if [ -z "${USE_EPICS_IOC}" -o "${USE_EPICS_IOC}" == 0 ]; then
          # Start caproto "black-hole" IOC:
          echo -e "\n" | caproto-spoof-beamline &

          # Solve a missing "caRepeater" binary (same reason as in
          # https://github.com/bluesky/tutorial/blob/master/binder/postBuild#L14-L15):
          ln -sv caproto-repeater ${CONDA_PREFIX}/bin/caRepeater
      else
          echo "Not using caproto-spoof-beamline IOC as instructed by .ci/bl-specific.sh"
      fi

      # Monkey-patch the default timeout for the ophyd signals:
      if [ "$OPHYD_CONTROL_LAYER" == "pyepics" ]; then
          connection_timeout="
      import ophyd
      import functools
      ophyd.signal.EpicsSignalBase.wait_for_connection = functools.partialmethod(ophyd.signal.EpicsSignalBase.wait_for_connection, timeout=${OPHYD_TIMEOUT})
      print(ophyd.signal.EpicsSignalBase.wait_for_connection.__dict__)"
      fi

      # This is what IPython does internally to load the startup files:
      command="${connection_timeout}
      import os
      import glob
      ip = get_ipython()
      startup_files = sorted(glob.glob('startup/*.py'))
      if os.path.isfile('.ci/drop-in.py'):
          startup_files.append('.ci/drop-in.py')
      if not startup_files:
          raise SystemExit(f'Cannot find any startup files in {os.getcwd()}')
      for f in startup_files:
          print(f'Executing {f} in CI')
          ip.parent._exec_file(f)"

      echo "$command"

      # TODO: remove the condition below once all repos are compliant.
      # This is a temporary step, and is needed for the non-compliant BL
      # profiles:
      # - https://github.com/NSLS-II-HXN/ipython_ophyd/tree/master/profile_collection
      # - https://github.com/NSLS-II-IXS/.ipython/tree/master/profile_collection
      # - https://github.com/NSLS-II-CSX/xf23id1_profiles/tree/master/profile_collection
      if [ -d "profile_collection/" ]; then
          cd profile_collection/
      fi

      # Start IPython startup files one by one using the generated command.
      # We don't load the startup files by IPython's means (with
      # "--profile=<profile-name>"), but rather load an empty test profile:
      ipython --profile=${TEST_PROFILE} -c "$command"

    displayName: "* optionally start caproto IOC and start IPython with startup files"
    condition: succeeded()

  # Display bluesky logs
  - script: |
      echo -e "\n\n=============================================================="
      cat $HOME/.cache/bluesky/log/bluesky.log
      echo -e "=============================================================="

      echo -e "\n\n=============================================================="
      cat $HOME/.cache/bluesky/log/bluesky_ipython.log
      echo -e "\n\n=============================================================="
    displayName: "* display bluesky logs"
    condition: succeededOrFailed()
