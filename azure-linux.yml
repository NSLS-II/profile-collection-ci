jobs:
- job: run_profile_collection_testing
  displayName: "profile_collection (${{ parameters.beamline_acronym }}):"

  variables:
    MPLBACKEND: Qt5Agg
    TEST_PROFILE: test
    BS_PROFILE: collection
    BS_ENV_VERSION: 2019C3.0.1

  strategy:
    matrix:
      pyepics_old_ch_py36:
        OPHYD_CONTROL_LAYER: pyepics
        CONDA_CHANNEL_NAME: lightsource2-tag
        PYTHON_VERSION: 3.6
      pyepics_new_ch_py36:
        OPHYD_CONTROL_LAYER: pyepics
        CONDA_CHANNEL_NAME: nsls2forge
        PYTHON_VERSION: 3.6
      caproto_old_ch_py36:
        OPHYD_CONTROL_LAYER: caproto
        CONDA_CHANNEL_NAME: lightsource2-tag
        PYTHON_VERSION: 3.6
      caproto_new_ch_py36:
        OPHYD_CONTROL_LAYER: caproto
        CONDA_CHANNEL_NAME: nsls2forge
        PYTHON_VERSION: 3.6
      pyepics_old_ch_py37:
        OPHYD_CONTROL_LAYER: pyepics
        CONDA_CHANNEL_NAME: lightsource2-tag
        PYTHON_VERSION: 3.7
      pyepics_new_ch_py37:
        OPHYD_CONTROL_LAYER: pyepics
        CONDA_CHANNEL_NAME: nsls2forge
        PYTHON_VERSION: 3.7
      caproto_old_ch_py37:
        OPHYD_CONTROL_LAYER: caproto
        CONDA_CHANNEL_NAME: lightsource2-tag
        PYTHON_VERSION: 3.7
      caproto_new_ch_py37:
        OPHYD_CONTROL_LAYER: caproto
        CONDA_CHANNEL_NAME: nsls2forge
        PYTHON_VERSION: 3.7

  steps:

  - script: env | sort -u
    displayName: check the env
    condition: succeeded()

  - script: |
      sudo apt-get install mongodb xvfb qtbase5-dev
    displayName: install required packages
    condition: succeeded()

  - script: |
      sudo systemctl start mongodb && sudo systemctl status mongodb
    displayName: start mongodb
    condition: succeeded()

  # Common steps:

  - script: |
      wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
      sh ./miniconda.sh -b -p ~/mc
      source ~/mc/etc/profile.d/conda.sh  # this makes all conda vars available
      conda update conda --yes
    displayName: download and install miniconda
    condition: succeeded()

  # pyOlog and databroker configs; prepare a profile dir:
  - script: |
      cp -v pyOlog-test.conf ~/pyOlog.conf

      mkdir -v -p ~/.config/databroker/
      cp -v databroker.yml ~/.config/databroker/$(git config --get remote.origin.url | awk -F'[/-]' '{print tolower($6)}').yml

      mkdir -v -p ~/.ipython/profile_${TEST_PROFILE}
    displayName: copy pyOlog and databroker configs; prepare a profile dir
    condition: succeeded()

  # Beamline-specific step:

  - script: |
      if [ -f .travis-local.sh ]; then
          bash .travis-local.sh
      fi
    displayName: perform beamline-specific actions
    condition: succeeded()

  # Create conda env:
  - script: |
      conda create -y -p ~/mc/envs/${BS_PROFILE}-${BS_ENV_VERSION} -c ${CONDA_CHANNEL_NAME} ${BS_PROFILE}=${BS_ENV_VERSION} python=${PYTHON_VERSION}
      conda env list
    displayName: create test conda env
    condition: succeeded()

  - script: |
      # Start Xvfb
      # (from https://developercommunity.visualstudio.com/content/problem/336288/headless-testing-using-xvfb-on-hosted-ubuntu-1604.html)
      /sbin/start-stop-daemon --start --quiet --pidfile /tmp/custom_xvfb_99.pid --make-pidfile --background --exec /usr/bin/Xvfb -- :99 -ac -screen 0 1280x1024x16
      export DISPLAY=:99

      # Activate the test env and display some relevant info:
      source ~/mc/etc/profile.d/conda.sh
      conda activate ${BS_PROFILE}-${BS_ENV_VERSION}
      conda env list
      conda info
      conda list

      # Start caproto "black-hole" IOC:
      echo -e "\n" | caproto-spoof-beamline &

      # Solve a missing "caRepeater" binary (same reason as in
      # https://github.com/bluesky/tutorial/blob/master/binder/postBuild#L14-L15):
      ln -sv caproto-repeater ${CONDA_PREFIX}/bin/caRepeater

      # Monkey-patch the default timeout for the ophyd signals:
      if [ "$OPHYD_CONTROL_LAYER" == "pyepics" ]; then
          connection_timeout='
      import ophyd
      import functools
      ophyd.signal.EpicsSignalBase.wait_for_connection = functools.partialmethod(ophyd.signal.EpicsSignalBase.wait_for_connection, timeout=20)
      print(ophyd.signal.EpicsSignalBase.wait_for_connection.__dict__)'
      fi

      # This is what IPython does internally to load the startup files:
      command="${connection_timeout}
      import glob
      ip = get_ipython()
      for f in sorted(glob.glob('startup/*.py')):
          print(f'Executing {f} in CI')
          ip.parent._exec_file(f)"

      echo "$command"

      # Start IPython startup files one by one using the generated command.
      # We don't load the startup files by IPython's means (with
      # "--profile=<profile-name>"), but rather load an empty test profile:
      EPICS_CA_AUTO_ADDR_LIST=NO EPICS_CA_ADDR_LIST=127.0.0.1 ipython --profile=${TEST_PROFILE} -c "$command"

    displayName: start caproto IOC and start IPython with startup files
    condition: succeeded()
